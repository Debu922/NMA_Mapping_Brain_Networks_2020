{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# Author(s): Debaditya, Anwesha, Anna                       #\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "from itertools import compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter path to data folder:\n",
    "data_root = '/NMA/Mapping Brain Networks/data/allData'\n",
    "\n",
    "#Get list of files in glob\n",
    "session_paths = glob.glob(data_root + '/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title groupings of brain regions\n",
    "regions = [\"vis ctx\", \"thal\", \"hipp\", \"other ctx\", \"midbrain\",  \"basal ganglia\", \"subplate\"]\n",
    "brain_groups = [[\"VISa\", \"VISam\", \"VISl\", \"VISp\", \"VISpm\", \"VISrl\"], # visual cortex\n",
    "                [\"CL\", \"LD\", \"LGd\", \"LH\", \"LP\", \"MD\", \"MG\", \"PO\", \"POL\", \"PT\", \"RT\", \"SPF\", \"TH\", \"VAL\", \"VPL\", \"VPM\"], # thalamus\n",
    "                [\"CA\", \"CA1\", \"CA2\", \"CA3\", \"DG\", \"SUB\", \"POST\"], # hippocampal\n",
    "                [\"ACA\", \"AUD\", \"COA\", \"DP\", \"ILA\", \"MOp\", \"MOs\", \"OLF\", \"ORB\", \"ORBm\", \"PIR\", \"PL\", \"SSp\", \"SSs\", \"RSP\",\" TT\"], # non-visual cortex\n",
    "                [\"APN\", \"IC\", \"MB\", \"MRN\", \"NB\", \"PAG\", \"RN\", \"SCs\", \"SCm\", \"SCig\", \"SCsg\", \"ZI\"], # midbrain\n",
    "                [\"ACB\", \"CP\", \"GPe\", \"LS\", \"LSc\", \"LSr\", \"MS\", \"OT\", \"SNr\", \"SI\"], # basal ganglia \n",
    "                [\"BLA\", \"BMA\", \"EP\", \"EPd\", \"MEA\"] # cortical subplate\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = [11,12] #Pick which sessions to load.\n",
    "dat = {}\n",
    "for session in sessions:\n",
    "    dat[session] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_info(root, path):\n",
    "    '''\n",
    "    This function returns the date on which the session was carried out and the name of the mouse.\n",
    "    \n",
    "    Args:\n",
    "    root - [string] The root directory path.\n",
    "    path - [string] The path to the session directory.\n",
    "    \n",
    "    Returns:\n",
    "    date - [string] Date the experiment was conducted in YYYY-MM-DD format.\n",
    "    name - [string] Name of the mouse\n",
    "    '''\n",
    "    \n",
    "    #Get substring\n",
    "    name_date = path.replace(data_root+'\\\\','')\n",
    "    \n",
    "    #Get date\n",
    "    date = name_date[:-11]\n",
    "    \n",
    "    #Get name\n",
    "    name = name_date[-10:]\n",
    "    \n",
    "    return date, name\n",
    "\n",
    "def get_cluster_info(path):\n",
    "    '''\n",
    "    This function returns information about clusters.\n",
    "    \n",
    "    Args:\n",
    "    path - [string] The path to the session directory.\n",
    "    \n",
    "    Returns:\n",
    "    good_clusters - [ndarray] Logical values representing if a cluster is 'good'.\n",
    "    brain_regions - [list] Location where a cluster is located.\n",
    "    '''\n",
    "    \n",
    "    #Get good clusters. _phy_annotation >=2\n",
    "    good_clusters = (np.load(path + '/clusters._phy_annotation.npy')>=2).flatten()\n",
    "    \n",
    "    #Get cluster_channels\n",
    "    cluster_channels = (np.load(path + '/clusters.peakChannel.npy').astype(int) - 1).flatten()\n",
    "    \n",
    "    #Create brain region temp variable\n",
    "    brain_regions = []\n",
    "    \n",
    "    #Open channel files\n",
    "    with open(path + '/channels.brainLocation.tsv') as tsvfile:\n",
    "        \n",
    "        reader = csv.DictReader(tsvfile, dialect='excel-tab')\n",
    "        \n",
    "        for row in reader:\n",
    "            \n",
    "            #Parse regions\n",
    "            brain_regions.append(row['allen_ontology'])\n",
    "    \n",
    "    #Create cluster location list.\n",
    "    cluster_locations = []\n",
    "    \n",
    "    #Iterate through the channels and parse the brain locations\n",
    "    for cluster_channel in cluster_channels:\n",
    "        \n",
    "        brain_region = brain_regions[cluster_channel]\n",
    "        cluster_locations.append(brain_region)\n",
    "        del brain_region\n",
    "            \n",
    "    #Return the variables.\n",
    "    return good_clusters, cluster_locations\n",
    " \n",
    "def get_cluster_spikes(path):\n",
    "    '''\n",
    "    This function retuns the spikes sorted according to clusters.\n",
    "    \n",
    "    args:\n",
    "    path - [string] The path to the session directory.\n",
    "    \n",
    "    return:\n",
    "    cluster_spikes - [list] This is a list of lists of spike timings.\n",
    "    '''\n",
    "    #Load the spikes\n",
    "    spikes = np.load(path + '/spikes.times.npy', allow_pickle = True).flatten()\n",
    "    \n",
    "    #load the cluster_ids\n",
    "    cluster_ids = np.load(path + '/spikes.clusters.npy', allow_pickle = True).flatten()\n",
    "    \n",
    "    #Create empty list\n",
    "    clusters_spikes = [] #NOTE I CHANGED THIS LOOK INTO THIS LATER!\n",
    "    \n",
    "    #iterate through cluster_ids to arrange spikes.\n",
    "    for cluster_id in range(np.max(cluster_ids)+1):\n",
    "        cluster_spikes = spikes[np.where(cluster_ids == cluster_id)]\n",
    "        clusters_spikes.append(cluster_spikes)\n",
    "        \n",
    "    #Return the variables.\n",
    "    return cluster_spikes\n",
    "\n",
    "def get_trial_info(path):\n",
    "    '''\n",
    "    This function returns all the information about the trials.\n",
    "    '''\n",
    "    trial_intervals = np.load(path + '/trials.intervals.npy', allow_pickle = True)\n",
    "    visualStim_times = np.load(path + '/trials.visualStim_times.npy', allow_pickle = True)\n",
    "    goCue_times = np.load(path + '/trials.goCue_times.npy', allow_pickle = True)\n",
    "    response_times = np.load(path + '/trials.response_times.npy', allow_pickle = True)\n",
    "    feedback_times = np.load(path + '/trials.feedback_times.npy', allow_pickle = True)\n",
    "    feedback_type =np.load(path + '/trials.feedbackType.npy', allow_pickle = True)\n",
    "    return trial_intervals, visualStim_times, goCue_times, response_times, feedback_times, feedback_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for session 11 loaded.\n",
      "Data for session 12 loaded.\n"
     ]
    }
   ],
   "source": [
    "#Load sessions\n",
    "dat = {}\n",
    "for session in sessions:\n",
    "    dat[session] = {}\n",
    "    path = session_paths[session]\n",
    "    \n",
    "    session_date, mouse_name = get_session_info(data_root, path)\n",
    "    good_clusters, cluster_locations = get_cluster_info(path)\n",
    "    cluster_spikes = get_cluster_spikes(path)\n",
    "    trial_intervals, visualStim_times, goCue_times, response_times, feedback_times, feedback_type = get_trial_info(path) \n",
    "    \n",
    "    dat[session]['session_date'] = session_date\n",
    "    dat[session]['mouse_name'] = mouse_name\n",
    "    dat[session]['good_clusters'] = good_clusters\n",
    "    dat[session]['cluster_locations'] = cluster_locations\n",
    "    dat[session]['cluster_spikes'] = cluster_spikes\n",
    "    dat[session]['trial_intervals'] = trial_intervals\n",
    "    dat[session]['visualStim_times'] = visualStim_times\n",
    "    dat[session]['goCue_times'] = goCue_times\n",
    "    dat[session]['response_times'] = response_times\n",
    "    dat[session]['feedback_times'] = feedback_times\n",
    "    dat[session]['feedback_type'] = feedback_type\n",
    "    \n",
    "    print('Data for session',session,'loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['session_date', 'mouse_name', 'good_clusters', 'cluster_locations', 'cluster_spikes', 'trial_intervals', 'visualStim_times', 'goCue_times', 'response_times', 'feedback_times', 'feedback_type'])\n"
     ]
    }
   ],
   "source": [
    "print(dat[11].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This part of the code can be used to save the data currently in memory\n",
    "\n",
    "def save_data(filename, objects):\n",
    "    '''\n",
    "    This function will save the data you want to into the named file\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    filename - [string] The name of the file that you want to save your data into. (include extention .pkl)\n",
    "    objects - [list] The list of objects you want to store from memory into the file.\n",
    "    \n",
    "    Return:\n",
    "    \n",
    "    void\n",
    "    \n",
    "    Usage Example:\n",
    "    save_data('data.pkl',[no_of_sessions ,spontaneous_intervals, trials_intervals, channel_brainLocations, clusters_phy_annotation, clusters_peakChannel, spikes_amps, spikes_clusters, spikes_depths, spikes_times])\n",
    "    '''\n",
    "    \n",
    "    #Grab dependencies\n",
    "    import pickle\n",
    "    \n",
    "    #Open file\n",
    "    with open(filename, 'wb') as f:\n",
    "        #Dump memory\n",
    "        pickle.dump(data, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #This cell loads all the intervals\n",
    "\n",
    "# #Load intervals\n",
    "# for session in sessions:\n",
    "    \n",
    "#     #The path to current session\n",
    "#     path = session_paths[session]\n",
    "    \n",
    "#     #The name & date\n",
    "#     session_date = get_name(data_root, path)\n",
    "#     mouse_name = get_date(data_root, path)\n",
    "    \n",
    "#     #List of cells which are good.\n",
    "#     good_cells = (np.load(path + '/clusters._phy_annotation.npy')>=2).flatten()\n",
    "#     cluster_channels = (np.load(path + '/clusters.peakChannel.npy').astype(int) - 1).flatten()\n",
    "    \n",
    "# #     #Debug\n",
    "# #     print(good_cells)\n",
    "# #     print(cluster_channels)\n",
    "# #     print(np.shape(cluster_channels),print(np.shape(good_cells)))\n",
    "# #     print(np.shape(cluster_channels[good_cells].flatten()))\n",
    "    \n",
    "# #     brain_regions = []\n",
    "# #     with open(path + '/channels.brainLocation.tsv') as tsvfile:\n",
    "# #         reader = csv.DictReader(tsvfile, dialect='excel-tab')\n",
    "# #         for row in reader:\n",
    "# #             brain_regions.append(row['allen_ontology'])\n",
    "# #         del reader\n",
    "    \n",
    "# #     #Debug\n",
    "# #     print(np.shape(brain_regions))\n",
    "# #     print(brain_regions)\n",
    "    \n",
    "\n",
    "#     cluster_locations = []\n",
    "#     for cluster_channel in cluster_channels:\n",
    "#         brain_region = brain_regions[cluster_channel]\n",
    "#         cluster_locations.append(brain_region)\n",
    "#         del brain_region\n",
    "    \n",
    "# #     #Debug\n",
    "# #     print(np.shape(cluster_locations))\n",
    "# #     print(len(list(compress(cluster_channels,good_cells))))\n",
    "# #     print(np.sum(good_cells))\n",
    "        \n",
    "#     spikes = np.load(path + '/spikes.times.npy', allow_pickle = True).flatten()\n",
    "#     cluster_ids = np.load(path + '/spikes.clusters.npy', allow_pickle = True).flatten()\n",
    "    \n",
    "# #     #Debug\n",
    "# #     print(np.min(cluster_ids))\n",
    "# #     print(np.max(cluster_ids))\n",
    "    \n",
    "#     clusters_spikes = [] #NOTE I CHANGED THIS LOOK INTO THIS LATER!\n",
    "#     for cluster_id in range(np.max(cluster_ids)+1):\n",
    "#         cluster_spikes = spikes[np.where(cluster_ids == cluster_id)]\n",
    "#         clusters_spikes.append(cluster_spikes)\n",
    "#         del cluster_spikes\n",
    "    \n",
    "# #     #Debug\n",
    "# #     print(type(clusters_spikes))\n",
    "# #     print(len(clusters_spikes))\n",
    "# #     print(clusters_spikes[np.max(cluster_ids)+1])\n",
    "    \n",
    "#     trial_intervals = np.load(path + '/trials.intervals.npy', allow_pickle = True)\n",
    "#     no_of_trials = len(trial_intervals)\n",
    "#     visualStim_times = np.load(path + '/trials.visualStim_times.npy', allow_pickle = True)\n",
    "#     goCue_times = np.load(path + '/trials.goCue_times.npy', allow_pickle = True)\n",
    "#     response_times = np.load(path + '/trials.response_times.npy', allow_pickle = True)\n",
    "#     feedback_times = np.load(path + '/trials.feedback_times.npy', allow_pickle = True)\n",
    "#     feedback_type =np.load(path + '/trials.feedbackType.npy', allow_pickle = True)\n",
    "    \n",
    "#     dat[session]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# #     print(no_of_trials)\n",
    "# #     trial_start_t = trial_intervals[:,0]\n",
    "# #     trial_end_t = trial_intervals[:,1]\n",
    "# #     trials_spikes = []\n",
    "# #     for trial in range(1):\n",
    "# #         trial_spikes = []\n",
    "# #         trial_start = trial_intervals[trial,0]\n",
    "# #         trial_end = trial_intervals[trial,1]\n",
    "        \n",
    "# #         for cluster_id in cluster_ids:\n",
    "# #             cluster_spikes = clusters_spikes[cluster_id]\n",
    "# #             response_in_trial = np.logical_and(cluster_spikes>=trial_start,cluster_spikes<trial_end)\n",
    "# #             trial_spikes.append(np.array(compress(cluster_spikes,response_in_trial)))\n",
    "# #             del cluster_spikes\n",
    "# #             del response_in_trial\n",
    "# #         trials_spikes.append(trial_spikes)\n",
    "# #         del trial_spikes\n",
    "# #         print(\"Finished grouping spikes for trial\",trial)\n",
    "# #             #print(type(cluster_spikes_trial))\n",
    "# #     trial_intervals = np.load(path + '/trials.intervals.npy', allow_pickle = True)\n",
    "# #     for trial_no in range(len(trial_intervals)):\n",
    "# #         trial_start = trial_intervals[trial_no,0]\n",
    "# #         trial_end = trial_intervals[trial_no,1]\n",
    "# #         trial_spikes = {}\n",
    "# #         for cell_id, cell_spike in enumerate(cell_spikes):\n",
    "# #             trial_spike = cell_spike[cell_spike>=trial_start]\n",
    "# #             trial_spike = trial_spike[trial_spike<trial_end]\n",
    "# #             trial_spikes[cell_id] = cell_spike[cell_spike>=trial_start and cell_spike>trial_end]\n",
    "# #         dat[session]['trial_spikes'].append(trial_spikes)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
